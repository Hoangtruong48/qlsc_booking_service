package com.qlsc.qlsc_booking_service.consumer;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.qlsc.qlsc_booking_service.dto.ProcessKafkaMessage;
import com.qlsc.qlsc_booking_service.entity.BlackFridaySale;
import com.qlsc.qlsc_booking_service.repo.custom.BlackFridaySaleCustomRepo;
import com.qlsc.qlsc_booking_service.service.BookingService;
import com.qlsc.qlsc_common.constant.KafkaConstant;
import com.qlsc.qlsc_common.event.TestEvent;
import com.qlsc.qlsc_common.saga.BlackFridaySaleCommand;
import com.qlsc.qlsc_common.saga.CreateBookingCommand;
import lombok.AccessLevel;
import lombok.RequiredArgsConstructor;
import lombok.experimental.FieldDefaults;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.messaging.handler.annotation.Payload;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.Objects;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@FieldDefaults(level = AccessLevel.PRIVATE, makeFinal = true)
public class BookingConsumer {
    Logger LOG = LoggerFactory.getLogger(this.getClass());
    BookingService bookingService;
    ObjectMapper objectMapper = new ObjectMapper();
    ExecutorService kafkaWorkerPool;
    BlackFridaySaleCustomRepo blackFridaySaleCustomRepo;
    private final AtomicInteger totalMessageCounter = new AtomicInteger(0);
    private final AtomicLong totalTimeCounter = new AtomicLong(0);
    private static final int TARGET_MESSAGES = 5000;
//    @KafkaListener(topics = KafkaConstant.TOPIC_BOOKING_EVENT,
//            groupId = "booking-group")
//    public void consume(
//            @Payload TestEvent event,
//            @Header("kafka_receivedPartitionId") int partition,
//            @Header("kafka_offset") long offset
//    ) {
//        LOG.info("üì© Received event: {} | partition= {} | offset={}", event, partition, offset);
//    }

    @KafkaListener(topics = KafkaConstant.TOPIC_BOOKING_COMMAND, groupId = KafkaConstant.GROUP_BOOKING)
    public void consumeBookingCommand(String payLoad) {
        try {
            CreateBookingCommand cmd = objectMapper.readValue(payLoad, CreateBookingCommand.class);
            bookingService.createBooking(cmd);
        } catch (Exception ex) {
            LOG.error("Error parsing CreateBookingCommand", ex);
        }
    }

    @KafkaListener(topics = KafkaConstant.TOPIC_BOOKING_MULTI_THREAD, groupId = "test-1", containerFactory = "batchFactory")
    public void listenBatch(List<ConsumerRecord<String, String>> records, Acknowledgment ack) {
        long startTime = System.currentTimeMillis();
        LOG.info("List listener size = {}", records.size());

        // 1. Ph√¢n ph√°t vi·ªác Parse JSON v√† chuy·ªÉn ƒë·ªïi cho c√°c Worker Thread
        List<CompletableFuture<ProcessKafkaMessage<BlackFridaySale>>> futures = records.stream()
                .map(record -> CompletableFuture.supplyAsync(() -> {
                    try {
                        // T·ª± tay d√πng ObjectMapper parse chu·ªói JSON th√†nh Object
                        String jsonPayload = record.value();
                        BlackFridaySaleCommand command = objectMapper.readValue(jsonPayload, BlackFridaySaleCommand.class);

//                        LOG.info("ƒêang x·ª≠ l√Ω userId: {}", command.getUserId());
                        BlackFridaySale b =  convertCommandToEntity(command);
                        ProcessKafkaMessage<BlackFridaySale> processKafkaMessage = new ProcessKafkaMessage<>();
                        if (b != null) {
                            processKafkaMessage.setStatus(ProcessKafkaMessage.SUCCESS);
                            processKafkaMessage.setDataSuccess(b);
                        } else {
                            processKafkaMessage.setStatus(ProcessKafkaMessage.FAILURE);
                            processKafkaMessage.setMessageFailure(jsonPayload);
                        }
                        return processKafkaMessage;

                    } catch (Exception e) {
                        LOG.error("L·ªói parse JSON t·∫°i offset {}. N·ªôi dung: {}", record.offset(), record.value(), e);
                        ProcessKafkaMessage<BlackFridaySale> processKafkaMessage = new ProcessKafkaMessage<>();
                        processKafkaMessage.setStatus(ProcessKafkaMessage.FAILURE);
                        processKafkaMessage.setMessageFailure(record.value());
                        return processKafkaMessage;
//                        return null;
                    }
                }, kafkaWorkerPool))
                .toList();

        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

        List<ProcessKafkaMessage<BlackFridaySale>> lstEntity = futures.stream()
                .map(CompletableFuture::join)
                .filter(Objects::nonNull)
                .toList();

        // 4. Batch Insert xu·ªëng DB
        if (!lstEntity.isEmpty()) {

            List<BlackFridaySale> lstInsert = lstEntity.stream()
                    .filter(x -> x.getStatus() == ProcessKafkaMessage.SUCCESS)
                    .map(ProcessKafkaMessage::getDataSuccess)
                    .filter(Objects::nonNull)
                    .toList();

            List<String> lstMessageFailure = lstEntity.stream()
                    .filter(x -> x.getStatus() == ProcessKafkaMessage.FAILURE)
                    .map(ProcessKafkaMessage::getMessageFailure)
                    .filter(Objects::nonNull)
                    .toList();

            try {
                blackFridaySaleCustomRepo.insertBatchSuccessAndFailure
                        (lstInsert, lstMessageFailure, KafkaConstant.TOPIC_BOOKING_MULTI_THREAD);
            } catch (Exception e) {
                LOG.error("L·ªói nghi√™m tr·ªçng khi insert batch xu·ªëng DB!", e);
                throw e;
            }



        } else {
            LOG.warn("Kh√¥ng c√≥ b·∫£n ghi n√†o h·ª£p l·ªá ƒë·ªÉ insert trong l√¥ n√†y.");
        }

        // 5. C·ª∞C K·ª≤ QUAN TR·ªåNG: Di chuy·ªÉn ack.acknowledge() XU·ªêNG D∆Ø·ªöI C√ôNG
        ack.acknowledge();

        long endTime = System.currentTimeMillis();
        long batchDuration = endTime - startTime;

        long accumulatedTime = totalTimeCounter.addAndGet(batchDuration);
        int currentTotalMessages = totalMessageCounter.addAndGet(records.size());

        if (currentTotalMessages >= TARGET_MESSAGES) {
            LOG.info("\n=======================================================");
            LOG.info("------------> K·∫æT QU·∫¢ BENCHMARK HO√ÄN T·∫§T");
            LOG.info("- T·ªïng s·ªë messages ƒë√£ x·ª≠ l√Ω : {}", currentTotalMessages);
            LOG.info("- T·ªîNG TH·ªúI GIAN TH·ª∞C THI   : {} ms", accumulatedTime);
            LOG.info("=======================================================\n");

            // (T√πy ch·ªçn) Reset l·∫°i ƒë·ªÉ n·∫øu b·∫°n b·∫Øn ti·∫øp 5000 c√°i n·ªØa th√¨ n√≥ ƒëo l·∫°i t·ª´ ƒë·∫ßu m√† kh√¥ng c·∫ßn restart app
            totalMessageCounter.set(0);
            totalTimeCounter.set(0);
        }
    }

    private BlackFridaySale convertCommandToEntity(BlackFridaySaleCommand command) {
        BlackFridaySale entity = new BlackFridaySale();
        if (command.getUserId() % 2 != 0) {
            return null;
        }
        entity.setUserId(command.getUserId());
        entity.setMsg(command.getMsg());
        return entity;
    }

    private void insertToDatabase(ConsumerRecord<String, BlackFridaySaleCommand> record) {

    }
}
